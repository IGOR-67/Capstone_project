# RL Experiments on Pendulum-v1: SAC vs PPO and Gamma Sweep

Этот репозиторий содержит воспроизводимые эксперименты по обучению агента управления маятником (Gymnasium: Pendulum-v1) с использованием Stable-Baselines3.

## Содержание
- Описание экспериментов
- Быстрый старт
- Установка и зависимости
- Запуск экспериментов
- Визуализация и проверка артефактов
- Структура артефактов
- Результаты и комментарии
- Тонкая настройка и советы
- Известные предупреждения и оговорки
- Лицензия

## Описание экспериментов

### Эксперимент 1: сравнение алгоритмов
- SAC vs PPO на одной среде, с сопоставимой архитектурой сети (MLP 256-256)
- Гипотеза: SAC сойдётся быстрее и стабильнее при тех же шагах; PPO требует большего бюджета и тонкой настройки

### Эксперимент 2: влияние gamma в SAC
- gamma = 0.95 vs 0.999 (остальные гиперпараметры фиксированы)
- Гипотеза: большая gamma слегка улучшит итоговую награду (более "дальнее" планирование), но замедлит начальную сходимость

### Среда
- Gymnasium: Pendulum-v1 (непрерывное действие, быстрая отладка на CPU)

## Быстрый старт

### Локально (Python 3.10+)


### В Google Colab
- откройте ноутбук, установите зависимости
- при желании укажите путь сохранения на Google Drive
- запустите скрипт

После запуска вы получите:
- графики наград (по каждому запуску и общий)
- видео финального/лучшего агента (mp4)
- csv- и json-отчёты с количественными метриками

## Установка и зависимости

Минимальный набор:
- Python 3.10+
- gymnasium
- stable-baselines3
- matplotlib
- pandas

Установка:

### Пример requirements.txt:
### gymnasium
### stable-baselines3
### matplotlib
### pandas


Параметры по умолчанию:
- total_timesteps = 120_000 на запуск
- оценка по 20 эпизодам
- сохранение графиков, видео, моделей и сводных метрик

## Визуализация и проверка артефактов

Глобальный график (в корневой папке артефактов):
- comparison_rewards.png - кривые средней награды (скользящее среднее) для всех запусков на одном рисунке

График для каждого запуска:
- <BASE_DIR>/<RUN_NAME>/run_reward_plot.png

Видео финального агента:
- <BASE_DIR>/<RUN_NAME>/videos/<name>demo<timestamp>-episode-1.mp4

Количественная оценка (mean ± std по 20 эпизодам):
- печатается в консоль при завершении запуска
- сохраняется в:
  - <BASE_DIR>/<RUN_NAME>/results.json (final_eval_mean_20, final_eval_std_20)
  - <BASE_DIR>/summary_results.csv (столбцы final_mean@20, final_std@20)

## Структура артефактов

После запуска:
- <BASE_DIR>/
  - comparison_rewards.png - общий график по всем запускам
  - summary_results.csv - сводная таблица метрик
  - <RUN_NAME>/ (пример: exp1_SAC_default)
    - monitor.csv - лог эпизодов (обучение)
    - run_reward_plot.png - график средней награды vs timestep
    - eval/
      - monitor.csv - лог оценок (EvalCallback)
    - models/
      - best_model.zip - лучшая модель по метрике EvalCallback
      - <RUN_NAME>_final.zip - финальная сохранённая модель
    - videos/
      - <RUN_NAME>demo<timestamp>-episode-1.mp4 - видео эпизода
    - config.json - конфигурация запуска
    - results.json - сводка метрик запуска (в т.ч. оценка по 20 эпизодам)

## Результаты и комментарии

Пример результатов на CPU (120k шагов, 1 сид; ближе к 0 - лучше):
- exp1_SAC_default: около -145 ± 70
- exp1_PPO_default: около -560 ± 97
- exp2_SAC_gamma_0p95: около -142 ± 71
- exp2_SAC_gamma_0p999: около -145 ± 69

Интерпретация:
- SAC vs PPO: SAC значительно лучше PPO при таком бюджете. PPO, как on-policy метод, обычно требует большего числа шагов, нормализации (VecNormalize), иной настройки n_steps/learning_rate/ent_coef
- Gamma в SAC: разница между γ=0.95 и γ=0.999 мала относительно дисперсии при 120k шагов. В таких условиях существенного преимущества не выявлено; для строгого вывода нужны больший бюджет и усреднение по нескольким сидом

## Тонкая настройка и советы

Для улучшения результатов PPO рекомендуется:
- Увеличить общий бюджет шагов
- Использовать нормализацию наблюдений (VecNormalize)
- Настроить гиперпараметры: n_steps, learning_rate, ent_coef
- Провести поиск по сетке гиперпараметров

## предупреждения 

- Результаты могут варьироваться в зависимости от random seed
- PPO демонстрирует высокую чувствительность к гиперпараметрам
- Для статистической значимости рекомендуется multiple runs с разными сидами
- Результаты получены на CPU, на GPU могут незначительно отличаться

## Лицензия

MIT License






